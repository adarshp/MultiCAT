{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Change the line below to point to the correct location \n",
    "# of the MultiCAT SQLite3 database\n",
    "con = sqlite3.connect(\"multicat.db\")\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * from utterance\", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLC_trials = [\"T000603\",\n",
    "              \"T000604\",\n",
    "              \"T000605\",\n",
    "              \"T000606\",\n",
    "              \"T000607\",\n",
    "              \"T000608\",\n",
    "              \"T000613\",\n",
    "              \"T000627\",\n",
    "              \"T000628\",\n",
    "              \"T000631\",\n",
    "              \"T000632\",\n",
    "              \"T000633\",\n",
    "              \"T000634\",\n",
    "              \"T000635\",\n",
    "              \"T000636\",\n",
    "              \"T000637\",\n",
    "              \"T000638\",\n",
    "              \"T000671\",\n",
    "              \"T000713\",\n",
    "              \"T000714\",\n",
    "              \"T000715\",\n",
    "              \"T000716\",\n",
    "              \"T000719\",\n",
    "              \"T000720\",\n",
    "              \"T000723\",\n",
    "              \"T000724\",\n",
    "              \"T000727\",\n",
    "              \"T000728\",\n",
    "              \"T000729\",\n",
    "              \"T000730\",\n",
    "              \"T000737\",\n",
    "              \"T000738\"]\n",
    "len(CLC_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'trial_id', and pickup trials with clc annotation\n",
    "grouped = df.groupby('trial')\n",
    "\n",
    "clc_dfs = {}\n",
    "\n",
    "for trial_id, group_df in grouped:\n",
    "    if trial_id in CLC_trials:\n",
    "        clc_dfs[trial_id] = group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ayesha_test_split = [\"T000605\",\n",
    "                    \"T000606\",\n",
    "                    \"T000671\",\n",
    "                    \"T000672\",\n",
    "                    \"T000625\",\n",
    "                    \"T000626\",\n",
    "                    \"T000727\",\n",
    "                    \"T000728\",\n",
    "                    \"T000737\",\n",
    "                    \"T000738\",\n",
    "                    \"T000609\",\n",
    "                    \"T000610\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = [clc_dfs[trial] for trial in CLC_trials if any(trial_id in trial for trial_id in Ayesha_test_split)]\n",
    "train_files = [clc_dfs[trial] for trial in CLC_trials if trial not in Ayesha_test_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude pre-game and post-game lines, keep the columns that will be used for baseline model\n",
    "\n",
    "def readAnn(df):\n",
    "    data = df[[\"participant\",\n",
    "                \"start_timestamp\",\n",
    "                \"asr_text\",\n",
    "                \"corrected_text\",\n",
    "                \"clc_label\"]]\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    # Find the first index where 'clc_label' is not None (start of the game)\n",
    "    start_index = data[data['clc_label'].notnull()].index.min()\n",
    "    # Find the last index where 'clc_label' is not None (end of the game)\n",
    "    end_index = data[data['clc_label'].notnull()].index.max()\n",
    "    # Slice the DataFrame to keep only rows between start_index and end_index\n",
    "    data = data.iloc[start_index:end_index]\n",
    "    # add a new column \"notes\" to store the boundary of each trail.\n",
    "    data['notes'] = None\n",
    "    data = data.append({'notes': 'end-of-the-trail'}, ignore_index=True)\n",
    "    data = data.fillna('NA')\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = [readAnn(df) for df in train_files]\n",
    "test_df = [readAnn(df) for df in test_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The training set contains\", len(train_df), \"files.\")\n",
    "print(\"The testing set contains\", len(test_df), \"files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine corr_utt with utt column. If corr_utt, replace utt with corr_utt.\n",
    "\n",
    "def replaceUtt(row):\n",
    "    if row[\"corrected_text\"] != \"NA\":\n",
    "        return row[\"corrected_text\"]\n",
    "    else:\n",
    "        return row[\"asr_text\"]\n",
    "\n",
    "for df in train_df:\n",
    "    df[\"asr_text\"] = df.apply(replaceUtt, axis=1)\n",
    "    df.drop('corrected_text', axis=1, inplace=True)\n",
    "\n",
    "for df in test_df:\n",
    "    df[\"asr_text\"] = df.apply(replaceUtt, axis=1)\n",
    "    df.drop('corrected_text', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine files to only two dataframe: train and test\n",
    "train_data = pd.concat(train_df, ignore_index=True)\n",
    "test_data = pd.concat(test_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column \"utt_id\" with unique identifiers for train and test data\n",
    "train_data['utt_id'] = [f'utt_{i+1}' for i in range(len(train_data))]\n",
    "test_data['utt_id'] = [f'utt_{i+1}' for i in range(len(test_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The training set contains\", len(train_data), \"utterances.\")\n",
    "print(\"The testing set contains\", len(test_data), \"utterances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the utt tokens\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For step 1\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    lemmatized_text = \" \".join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "\n",
    "train_data[\"lemmatized_text\"] = train_data[\"asr_text\"].apply(lemmatize_text)\n",
    "test_data[\"lemmatized_text\"] = test_data[\"asr_text\"].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give \"a\" label and \"not-a\" label to the utterance\n",
    "\n",
    "def find_a_label(label):\n",
    "    if \"a\" in label:\n",
    "        return \"a\"\n",
    "    else:\n",
    "        return \"not-a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"step1_label\"] = train_data[\"clc_label\"].apply(find_a_label)\n",
    "test_data[\"step1_label\"] = test_data[\"clc_label\"].apply(find_a_label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Detect the \"a\" Stage\n",
    "\n",
    "using logistic regression with TF-IDF vector features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn the vocabulary and idf values\n",
    "\n",
    "def run_vectorizer(vectorizer, train, test):\n",
    "    return vectorizer.fit_transform(train), vectorizer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x = run_vectorizer(vectorizer, train_data[\"lemmatized_text\"], test_data[\"lemmatized_text\"])\n",
    "print(f\"Shape of train input data: {train_x.get_shape()}\")\n",
    "print(f\"Shape of test input data: {test_x.get_shape()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation\n",
    "model = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "\n",
    "def run_model(model, train_x, train_y, test_x):\n",
    "    # fit the model to the training data\n",
    "    model.fit(train_x, train_y)\n",
    "    # make predictions on the test data\n",
    "    pred_y = model.predict(test_x)\n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = run_model(model, train_x, train_data[\"step1_label\"], test_x)\n",
    "test_data['step1_prediction'] = prediction\n",
    "test_data[['asr_text', 'lemmatized_text', 'step1_label', 'step1_prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Detect a stage\")\n",
    "print(f\"accuracy: {accuracy_score(test_data['step1_label'], test_data['step1_prediction']):0.4f}\")\n",
    "print(f\"binary f1: {f1_score(test_data['step1_label'], test_data['step1_prediction'], average='binary', pos_label='a'):0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the balance of dataset\n",
    "\n",
    "train_data.groupby(by=\"step1_label\")[[\"asr_text\"]].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Detect \"b\" Stages within a Window for Each \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2 need a dev dataset for the RoBERTa model, so that we neeed to split the train_files\n",
    "Ayesha_dev_split = [\"T000613\",\n",
    "                    \"T000607\",\n",
    "                    \"T000608\",\n",
    "                    \"T000633\",\n",
    "                    \"T000634\",\n",
    "                    \"T000614\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_files = [clc_dfs[trial] for trial in CLC_trials if any(trial_id in trial for trial_id in Ayesha_dev_split)]\n",
    "step2_train_files = [clc_dfs[trial] for trial in CLC_trials if trial not in (Ayesha_test_split+Ayesha_dev_split)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = [readAnn(df) for df in dev_files]\n",
    "step2_train_df = [readAnn(df) for df in step2_train_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dev set contains\", len(dev_df), \"files.\")\n",
    "print(\"The training set contains\", len(step2_train_df), \"files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dev_df:\n",
    "    df[\"asr_text\"] = df.apply(replaceUtt, axis=1)\n",
    "    df.drop('corrected_text', axis=1, inplace=True)\n",
    "\n",
    "for df in step2_train_df:\n",
    "    df[\"asr_text\"] = df.apply(replaceUtt, axis=1)\n",
    "    df.drop('corrected_text', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine files to only two dataframe: dev and step2_train\n",
    "dev_data = pd.concat(dev_df, ignore_index=True)\n",
    "step2_train_data = pd.concat(step2_train_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column \"utt_id\" with unique identifiers for train and test data\n",
    "dev_data['utt_id'] = [f'utt_{i+1}' for i in range(len(dev_data))]\n",
    "step2_train_data['utt_id'] = [f'utt_{i+1}' for i in range(len(step2_train_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dev set contains\", len(dev_data), \"utterances.\")\n",
    "print(\"The training set contains\", len(step2_train_data), \"utterances.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching utterances\n",
    "For each stage \"a\" utterance, find 3 utterances following that \"a\" utterance which produced by speaker other than the speaker of \"a\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set window size for looking for \"b\"\n",
    "window_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split combined files into trials\n",
    "\n",
    "def splittingTrials(combined_df):\n",
    "    # Find the indices where \"notes\" is \"end of the trial\"\n",
    "    trial_indices = combined_df[combined_df['notes'] == \"end-of-the-trail\"].index\n",
    "\n",
    "    # Initialize a list to store DataFrames for each trial\n",
    "    trial_dataframes = []\n",
    "\n",
    "    # Initialize a variable to keep track of the previous \"end of the trial\" index\n",
    "    prev_trial_end = 0\n",
    "\n",
    "    # Loop through trial indices to split the DataFrame\n",
    "    for trial_end in trial_indices:\n",
    "        # Slice the DataFrame to extract the trial data\n",
    "        trial_df = combined_df.iloc[prev_trial_end:trial_end + 1]\n",
    "        trial_df.reset_index(drop=True, inplace=True)\n",
    "        # Append the trial DataFrame to the list\n",
    "        trial_dataframes.append(trial_df)\n",
    "\n",
    "        # Update the previous trial end index for the next iteration\n",
    "        prev_trial_end = trial_end + 1\n",
    "\n",
    "    # print the number of trials\n",
    "    print(f\"Number of trials: {len(trial_dataframes)}\")\n",
    "\n",
    "    return trial_dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_train = splittingTrials(step2_train_data)\n",
    "splitted_dev = splittingTrials(dev_data)\n",
    "splitted_test = splittingTrials(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training, Dev, and Testing Data for step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitLabel(label):\n",
    "    # For NA labels\n",
    "    if label.strip() == \"NA\":\n",
    "        return [[\"NA\", \"NA\"]]\n",
    "\n",
    "    splitted_labels = []\n",
    "    clc_labels = [i.rstrip(\"+\") for i in label.split(\".\")]\n",
    "    for label in clc_labels:\n",
    "        label = re.sub(r\"(?<=[a-zA-Z]).*$\", \"\", label)\n",
    "        turn_id, label = re.findall(r'\\d+|\\D+', label)\n",
    "        splitted_labels.append([turn_id, label])\n",
    "\n",
    "    return splitted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitLabel(\"1b+.2a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each stage \"a\" utterance, select {window size} utterance from speakers other than the speaker of \"a\"\n",
    "\n",
    "def findCandidate(data, utt_id, turn_id, window_size):\n",
    "    \"\"\"\n",
    "    Find a list of `utt_id` values for candidate utterances within a specified window after the input `utt_id`.\n",
    "\n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The DataFrame containing conversation data.\n",
    "    - utt_id (str): The `utt_id` of the \"a\" stage utterance.\n",
    "    - turn_id (str): The turn number of the \"a\" stage utterance.\n",
    "    - window_size (int): The size of the window to look forward for candidate utterances.\n",
    "\n",
    "    Returns:\n",
    "    - candidate_utt_ids (list): A list of `utt_id` values for candidate utterances.\n",
    "    - candidate_utt_labels (list): A list of labels for the candidate utterance(\"a\", 1, 0).\n",
    "    1 denotes \"b\" label, 0 denotes \"not-b\" label.\n",
    "    \"\"\"\n",
    "    candidate_utt_ids, candidate_utt_labels = [utt_id], [\"a\"]\n",
    "    len_data = len(data)\n",
    "    maxlen_out_list = window_size + 1\n",
    "    # Find the row index corresponding to the input `utt_id`\n",
    "    a_stage_row = data[data['utt_id'] == utt_id]\n",
    "\n",
    "    # Extract the participant from the input a_stage_row\n",
    "    participant = a_stage_row.iloc[0]['participant']\n",
    "\n",
    "    # Find the current index of the a_stage_row\n",
    "    current_index = a_stage_row.index[0]\n",
    "\n",
    "    if current_index == len_data - 1:\n",
    "        return [utt_id, \"[PAD]\", \"[PAD]\", \"[PAD]\"], [\"a\", 0, 0, 0]\n",
    "\n",
    "    # Iterate through the DataFrame from the current a_stage_row and onwards\n",
    "    for _, row in data.iloc[current_index + 1:].iterrows():\n",
    "        # Extract the participant from the current row\n",
    "        current_participant = row['participant']\n",
    "        # Extract the label and split it\n",
    "        current_splited_labels = splitLabel(row['clc_label'])\n",
    "\n",
    "        # Check if the current utterance is made by a different speaker and within maxlen_out_list\n",
    "        if current_participant != participant and len(candidate_utt_ids) < maxlen_out_list:\n",
    "            candidate_utt_ids.append(row['utt_id'])\n",
    "\n",
    "            # Check the turn and append the label\n",
    "            b_found = False\n",
    "            for splitted_label in current_splited_labels:\n",
    "                turn, label = splitted_label\n",
    "                if turn == turn_id and label == \"b\":\n",
    "                    b_found = True\n",
    "                    break\n",
    "\n",
    "            if b_found:\n",
    "                candidate_utt_labels.append(1)\n",
    "            else:\n",
    "                candidate_utt_labels.append(0)\n",
    "\n",
    "\n",
    "        # Check if the length of candidate_utt_ids has reached the maxlen_out_list\n",
    "        if len(candidate_utt_ids) == maxlen_out_list:\n",
    "            break  # Stop finding when the length reaches the maxlen_out_list\n",
    "\n",
    "    if len(candidate_utt_ids) < window_size:\n",
    "        candidate_utt_ids += [\"[PAD]\"] * (maxlen_out_list - len(candidate_utt_ids))\n",
    "        candidate_utt_labels += [0] * (maxlen_out_list - len(candidate_utt_labels))\n",
    "\n",
    "    return candidate_utt_ids, candidate_utt_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training data\n",
    "\n",
    "train_candidate_utt_ids, train_candidate_utt_labels = [], []\n",
    "\n",
    "for data in splitted_train:\n",
    "    for _, row in data.iterrows():\n",
    "        utt_id = row['utt_id']\n",
    "        splitted_labels = splitLabel(row['clc_label'])\n",
    "        for splitted_label in splitted_labels:\n",
    "            turn_id, label = splitted_label\n",
    "            if label == \"a\":\n",
    "                utt_ids, utt_labels = findCandidate(data, utt_id, turn_id, window_size)\n",
    "                train_candidate_utt_ids.append(utt_ids)\n",
    "                train_candidate_utt_labels.append(utt_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dev data\n",
    "\n",
    "dev_candidate_utt_ids, dev_candidate_utt_labels = [], []\n",
    "\n",
    "for data in splitted_dev:\n",
    "    for _, row in data.iterrows():\n",
    "        utt_id = row['utt_id']\n",
    "        splitted_labels = splitLabel(row['clc_label'])\n",
    "        for splitted_label in splitted_labels:\n",
    "            turn_id, label = splitted_label\n",
    "            if label == \"a\":\n",
    "                utt_ids, utt_labels = findCandidate(data, utt_id, turn_id, window_size)\n",
    "                dev_candidate_utt_ids.append(utt_ids)\n",
    "                dev_candidate_utt_labels.append(utt_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test data\n",
    "\n",
    "test_candidate_utt_ids, test_candidate_utt_labels = [], []\n",
    "\n",
    "for data in splitted_test:\n",
    "    for _, row in data.iterrows():\n",
    "        utt_id = row['utt_id']\n",
    "        splitted_labels = splitLabel(row['clc_label'])\n",
    "        for splitted_label in splitted_labels:\n",
    "            turn_id, label = splitted_label\n",
    "            if label == \"a\":\n",
    "                utt_ids, utt_labels = findCandidate(data, utt_id, turn_id, window_size)\n",
    "                test_candidate_utt_ids.append(utt_ids)\n",
    "                test_candidate_utt_labels.append(utt_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a row to the train_data with \"[PAD]\" values for \"utt_id\" and \"utt\"\n",
    "pad_row = {'utt_id': '[PAD]', 'asr_text': '[PAD]'}\n",
    "# Fill other keys with NaN for columns not specified in new_row\n",
    "pad_row.update({col: pd.NA for col in step2_train_data.columns if col not in pad_row})\n",
    "train_data_add_pad = train_data.append(pad_row, ignore_index=True)\n",
    "# for dev\n",
    "pad_row.update({col: pd.NA for col in dev_data.columns if col not in pad_row})\n",
    "dev_data_add_pad = dev_data.append(pad_row, ignore_index=True)\n",
    "# for testing\n",
    "pad_row.update({col: pd.NA for col in test_data.columns if col not in pad_row})\n",
    "test_data_add_pad = test_data.append(pad_row, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to store the data for step 2.\n",
    "# Columns: a_utt_id, a_utt, candidate_b_id, candidate_b_utt, label\n",
    "# The first three a_utt_id and a_utterance will be the same,\n",
    "# with different candidate candidate_b_utt and the labels for the candidate_b_utt.\n",
    "\n",
    "step2_train_data = []\n",
    "for utterance_id_group, label_group in zip(train_candidate_utt_ids, train_candidate_utt_labels):\n",
    "    a_utt_id = utterance_id_group[0]\n",
    "    a_utt_text = train_data_add_pad.loc[train_data_add_pad['utt_id'] == a_utt_id, 'asr_text'].values[0]\n",
    "    for candidate_b_utt_id, label in zip(utterance_id_group[1:], label_group[1:]):\n",
    "        candidate_b_text = train_data_add_pad.loc[train_data_add_pad['utt_id'] == candidate_b_utt_id, 'asr_text'].values[0]\n",
    "        step2_train_data.append([a_utt_id, a_utt_text, candidate_b_utt_id, candidate_b_text, label])\n",
    "\n",
    "# Columns: 'a_utt_id', 'a_utt', 'candidate_b_id', 'candidate_b_utt', 'label'\n",
    "step2_train_df = pd.DataFrame(step2_train_data, columns=['a_utt_id', 'a_utt', 'candidate_b_id', 'candidate_b_utt', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dev\n",
    "\n",
    "step2_dev_data = []\n",
    "for utterance_id_group, label_group in zip(dev_candidate_utt_ids, dev_candidate_utt_labels):\n",
    "    a_utt_id = utterance_id_group[0]\n",
    "    a_utt_text = dev_data_add_pad.loc[dev_data_add_pad['utt_id'] == a_utt_id, 'asr_text'].values[0]\n",
    "    for candidate_b_utt_id, label in zip(utterance_id_group[1:], label_group[1:]):\n",
    "        candidate_b_text = dev_data_add_pad.loc[dev_data_add_pad['utt_id'] == candidate_b_utt_id, 'asr_text'].values[0]\n",
    "        step2_dev_data.append([a_utt_id, a_utt_text, candidate_b_utt_id, candidate_b_text, label])\n",
    "\n",
    "# Columns: 'a_utt_id', 'a_utt', 'candidate_b_id', 'candidate_b_utt', 'label'\n",
    "step2_dev_df = pd.DataFrame(step2_dev_data, columns=['a_utt_id', 'a_utt', 'candidate_b_id', 'candidate_b_utt', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing data\n",
    "\n",
    "step2_test_data = []\n",
    "for utterance_id_group, label_group in zip(test_candidate_utt_ids, test_candidate_utt_labels):\n",
    "    a_utt_id = utterance_id_group[0]\n",
    "    a_utt_text = test_data_add_pad.loc[test_data_add_pad['utt_id'] == a_utt_id, 'asr_text'].values[0]\n",
    "    for candidate_b_utt_id, label in zip(utterance_id_group[1:], label_group[1:]):\n",
    "        candidate_b_text = test_data_add_pad.loc[test_data_add_pad['utt_id'] == candidate_b_utt_id, 'asr_text'].values[0]\n",
    "        step2_test_data.append([a_utt_id, a_utt_text, candidate_b_utt_id, candidate_b_text, label])\n",
    "\n",
    "# Columns: 'a_utt_id', 'a_utt', 'candidate_b_id', 'candidate_b_utt', 'label'\n",
    "step2_test_df = pd.DataFrame(step2_test_data, columns=['a_utt_id', 'a_utt', 'candidate_b_id', 'candidate_b_utt', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pre-trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          TrainingArguments, Trainer,\n",
    "                          enable_full_determinism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3  # Number of epochs to train the model\n",
    "batch_size = 16  # Number of examples used per gradient update\n",
    "learning_rate = 5e-5  # The learning rate for the optimizer\n",
    "max_length = 50  # Maximum lenght of the input sequence\n",
    "output_dir = \"Step2_model\"  # The output directory where the model will be written to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    '''This function takes the name of the pre-trained mode as input, returns\n",
    "    the loaded model, initialized for Text Classification, and its tokenizer.'''\n",
    "    # Load the pre-trained tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Load the pre-trained model for text classification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    # Return the model and tokenizer\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Dataset library for dataset management\n",
    "step2_train_data = step2_train_df.loc[:, ['a_utt', 'candidate_b_utt', 'label']]\n",
    "step2_dev_data = step2_dev_df.loc[:, ['a_utt', 'candidate_b_utt', 'label']]\n",
    "step2_test_data = step2_test_df.loc[:, ['a_utt', 'candidate_b_utt', 'label']]\n",
    "step2_train_dataset = Dataset.from_pandas(step2_train_data)\n",
    "step2_dev_dataset = Dataset.from_pandas(step2_dev_data)\n",
    "step2_test_dataset = Dataset.from_pandas(step2_test_data)\n",
    "step2_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples, tokenizer, max_length):\n",
    "    '''This function takes examples of the Dataset, and tokenize the utterance\n",
    "    pairs. Tokenizer is the output of the load_model.\n",
    "    The function run the tokenizer jointly on the a_utt and candidate_b_utt columns of the\n",
    "    Dataset. The tokenizer pad and truncate the sequences to the max_length.\n",
    "    Return: the output of the tokenizer.'''\n",
    "    tokenized_sents = tokenizer(\n",
    "        examples['a_utt'], examples['candidate_b_utt'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    return tokenized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2_train_dataset = step2_train_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length), batched=True)\n",
    "step2_dev_dataset = step2_dev_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length), batched=True)\n",
    "step2_test_dataset = step2_test_dataset.map(lambda x: preprocess_data(x, tokenizer, max_length), batched=True)\n",
    "print(step2_train_dataset[0])\n",
    "print(tokenizer.convert_ids_to_tokens(step2_train_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_arguments(epochs, batch_size, learning_rate, output_dir):\n",
    "    \"\"\"\n",
    "    Create a TrainingArguments object for a training session with the given hyperparameters.\n",
    "    Args:\n",
    "        epochs (int): The number of epochs to train for.\n",
    "        batch_size (int): The batch size to use during training.\n",
    "        learning_rate (float): The learning rate to use during training.\n",
    "        output_dir (str): The output directory to save the model and training artifacts.\n",
    "    Returns:\n",
    "        A TrainingArguments object with the specified hyperparameters.\n",
    "    \"\"\"\n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        save_strategy='no',\n",
    "        evaluation_strategy='epoch',\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        do_eval = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = create_training_arguments(epochs, batch_size, learning_rate, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(model, train_args, train_dataset, dev_dataset):\n",
    "    \"\"\"\n",
    "    Create a Trainer object for a training session with the given model, training arguments, and datasets.\n",
    "    Args:\n",
    "        model: The pre-trained or randomly initialized model to train.\n",
    "        training_args: The TrainingArguments object specifying the hyperparameters and settings for the training.\n",
    "        train_dataset: The dataset to use for training.\n",
    "        dev_dataset: The dataset to use for evaluation during training.\n",
    "    Returns:\n",
    "        A Trainer object with the specified model, training arguments, and datasets.\n",
    "    \"\"\"\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = create_trainer(model, train_args, step2_train_dataset,step2_dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(trainer, test_dataset):\n",
    "    \"\"\"\n",
    "    Use the trained model to make predictions on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        trainer: The Trainer object with the trained model.\n",
    "        test_dataset: The test dataset to make predictions on.\n",
    "\n",
    "    Returns:\n",
    "        A list of integers containing the predicted label index for each statement pair in the test dataset.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    # Run the predict method on the test dataset to get the logits for each label\n",
    "    output = trainer.predict(test_dataset)\n",
    "    logits = output.predictions\n",
    "\n",
    "    # Get the index of the label with the highest logit value for each statement pair\n",
    "    predicted_labels = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = make_predictions(trainer, step2_test_dataset)\n",
    "step2_test_df[\"step2_prediction\"] = predictions\n",
    "step2_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(test_data):\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    f1 = evaluate.load(\"f1\")\n",
    "    # Compute accuracy and F1 score\n",
    "    accuracy_result = accuracy.compute(predictions=test_data[\"step2_prediction\"].values, references=test_data[\"label\"].values)\n",
    "    f1_result = f1.compute(predictions=test_data[\"step2_prediction\"].values, references=test_data[\"label\"].values, average='macro')  # 'macro' can be changed based on your needs\n",
    "\n",
    "    # Return both accuracy and F1 score\n",
    "    return accuracy_result, f1_result\n",
    "\n",
    "evaluate_prediction(step2_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the structure of the DataFrame for aggregation\n",
    "columns = ['utt_id', 'step1_label', 'step1_prediction', 'step2_label', 'step2_prediction']\n",
    "\n",
    "# Create an empty DataFrame with these columns\n",
    "aggregated_test = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy 'utt_id' column from \"test_data\" to the new DataFrame\n",
    "aggregated_test['utt_id'] = test_data['utt_id']\n",
    "\n",
    "# Transform 'step1_label' and 'step1_prediction' values from \"a\" and \"not-a\" to 1 and 0 respectively\n",
    "aggregated_test['step1_label'] = test_data['step1_label'].map({'a': 1, 'not-a': 0})\n",
    "aggregated_test['step1_prediction'] = test_data['step1_prediction'].map({'a': 1, 'not-a': 0})\n",
    "\n",
    "aggregated_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the first element of each inner list in test_candidate_utt_labels with the corresponding element from test_candidate_utt_ids\n",
    "for i in range(len(test_candidate_utt_labels)):\n",
    "    test_candidate_utt_labels[i][0] = test_candidate_utt_ids[i][0]\n",
    "\n",
    "test_candidate_utt_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the updated test_candidate_utt_labels to update the 'step2_label' in the new DataFrame 'df'\n",
    "for utt_labels in test_candidate_utt_labels:\n",
    "    utt_id = utt_labels[0]  # Extract the utt_id\n",
    "    # Check if there's at least one 1 in the rest of the insider list\n",
    "    step2_label_value = 1 if 1 in utt_labels[1:] else 0\n",
    "    # Assign the value to the 'step2_label' column in 'df' for the corresponding 'utt_id'\n",
    "    aggregated_test.loc[aggregated_test['utt_id'] == utt_id, 'step2_label'] = step2_label_value\n",
    "\n",
    "aggregated_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group by 'a_utt_id' and calculate the sum of 'step2_prediction' for each group\n",
    "step2_prediction_sum = step2_test_df.groupby('a_utt_id')['step2_prediction'].sum().reset_index()\n",
    "\n",
    "step2_prediction_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add 'step2_prediction' column to 'aggregated_test' with default values\n",
    "aggregated_test['step2_prediction'] = 0  # Adding default value of 0\n",
    "\n",
    "# Now, find every 'utt_id' from 'step2_prediction_sum' in 'aggregated_test'\n",
    "# If 'step2_prediction' != 0, then 'step2_prediction' in 'aggregated_test' gets a value of 1, otherwise, it remains 0\n",
    "for index, row in step2_prediction_sum.iterrows():\n",
    "    if row['step2_prediction'] != 0:\n",
    "        aggregated_test.loc[aggregated_test['utt_id'] == row['a_utt_id'], 'step2_prediction'] = 1\n",
    "\n",
    "aggregated_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_test = aggregated_test.fillna(0)\n",
    "aggregated_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'y_test' to the DataFrame 'aggregated_test' based on the conditions specified\n",
    "aggregated_test['y_test'] = aggregated_test.apply(lambda row: \"NA\" if row['step1_label'] == 0 else (\"closed\" if row['step1_label'] == 1 and row['step2_label'] == 1 else \"open\"), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_test['y_pred'] = aggregated_test.apply(lambda row: \"NA\" if row['step1_prediction'] == 0 else (\"closed\" if row['step1_prediction'] == 1 and row['step2_prediction'] == 1 else \"open\"), axis=1)\n",
    "aggregated_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class_report = classification_report(aggregated_test['y_test'], aggregated_test['y_pred'], digits=3)\n",
    "class_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_weighted = f1_score(aggregated_test['y_test'], aggregated_test['y_pred'], average='weighted')  # Calculate weighted-average F1 score\n",
    "f1_weighted = round(f1_weighted, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_weighted = round(f1_weighted, 3)\n",
    "f1_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
